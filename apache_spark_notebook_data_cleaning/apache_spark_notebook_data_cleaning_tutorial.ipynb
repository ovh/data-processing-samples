{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f06e1d3-f9d7-4a65-8fa8-c5af16509ad3",
   "metadata": {},
   "source": [
    "# Tutorial: Data cleaning with Notebooks for Apache Spark\n",
    "\n",
    "The purpose of this tutorial is to illustrate a simple data cleaning workflow with `Notebooks for Apache Spark`.\n",
    "\n",
    "**USE CASE: data cleaning of 2 CSV dataset with aggregation into a single clean Parquet file.**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**What is Data Cleaning?**\n",
    "\n",
    "\"*Data Cleaning or Data Cleansing is the preparation of raw data by detecting and correcting records within a dataset.*\"\n",
    "\n",
    "In this tutorial, you will be able to leverage the power Apache Spark from within a Jupyter Notebook.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To use `Notebooks for Apache Spark` you need first to create a new notebook from the Control Panel.\n",
    "\n",
    "You will also need a `Standard Object Storage - S3 API` container for the datasets, preferably in the same region as the notebook.\n",
    "\n",
    "## Code\n",
    "\n",
    "The different steps are as follow:\n",
    "\n",
    "* Setup the environment\n",
    "* Initialize the Spark Session\n",
    "* Download and check the `books` dataset\n",
    "* Prepare the `books` dataset\n",
    "* Verify the cleaned `books` dataset\n",
    "* Download and check the `ratings` dataset\n",
    "* Prepare the `ratings` dataset\n",
    "* Aggregate the data with SQL\n",
    "* Check the aggregated data\n",
    "* Save the data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b6e32-089d-4ce1-b981-05cc9a6b3644",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7578b108-299d-4ef4-9130-36c239f20fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, length, isnan, when\n",
    "\n",
    "# Check PySpark version\n",
    "print('PySpark version: ' + spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db91921-e859-46dc-91d7-5e4ff5cb7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 settings\n",
    "#s3_access_key=\"<access_key>\"\n",
    "#s3_secret_key=\"<secret_key>\"\n",
    "#s3_endpoint=\"s3.<region>.io.cloud.ovh.net\"\n",
    "s3_access_key=\"2cdcf83ec812492caa8aadcbd6926f87\"\n",
    "s3_secret_key=\"518e81ddc698464e9630f427b032287e\"\n",
    "s3_endpoint=\"s3.gra.io.cloud.ovh.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ae8c3-5a79-4ca9-aded-71f5adf1f70b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "612ef134-8e43-467a-9bbb-dd140e4e937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"spark_notebook_data_cleaning_tuto\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key)\\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key)\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", s3_endpoint)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da074a-8c43-4644-9eef-f2ae2ff52897",
   "metadata": {},
   "source": [
    "### Download and check the \"books\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "957926e1-2404-4e6e-9c67-d6064534ba3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, book_id: string, best_book_id: string, work_id: string, books_count: string, isbn: string, isbn13: string, authors: string, original_publication_year: string, original_title: string, title: string, language_code: string, image_url: string, small_image_url: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading \"books\" data from the S3 container\n",
    "df_books = spark.read.options(header='true').csv('s3a://spark-notebook-s3-container/books.csv')\n",
    "display(df_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a37061e4-3c35-4abd-8701-4fdbd7460f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------\n",
      " id                        | 1                                                          \n",
      " book_id                   | 2767052                                                    \n",
      " best_book_id              | 2767052                                                    \n",
      " work_id                   | 2792775                                                    \n",
      " books_count               | 272                                                        \n",
      " isbn                      | 439023483                                                  \n",
      " isbn13                    | 9.78043902348e+12                                          \n",
      " authors                   | Suzanne Collins                                            \n",
      " original_publication_year | 2008.0                                                     \n",
      " original_title            | The Hunger Games                                           \n",
      " title                     | The Hunger Games (The Hunger Games, #1)                    \n",
      " language_code             | eng                                                        \n",
      " image_url                 | https://images.gr-assets.com/books/1447303603m/2767052.jpg \n",
      " small_image_url           | https://images.gr-assets.com/books/1447303603s/2767052.jpg \n",
      "-RECORD 1-------------------------------------------------------------------------------\n",
      " id                        | 2                                                          \n",
      " book_id                   | 3                                                          \n",
      " best_book_id              | 3                                                          \n",
      " work_id                   | 4640799                                                    \n",
      " books_count               | 491                                                        \n",
      " isbn                      | 439554934                                                  \n",
      " isbn13                    | 9.78043955493e+12                                          \n",
      " authors                   | J.K. Rowling, Mary GrandPré                                \n",
      " original_publication_year | 1997.0                                                     \n",
      " original_title            | Harry Potter and the Philosopher's Stone                   \n",
      " title                     | Harry Potter and the Sorcerer's Stone (Harry Potter, #1)   \n",
      " language_code             | eng                                                        \n",
      " image_url                 | https://images.gr-assets.com/books/1474154022m/3.jpg       \n",
      " small_image_url           | https://images.gr-assets.com/books/1474154022s/3.jpg       \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at a couple of books\n",
    "df_books.show(2, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "335f0f7c-6a92-44b3-b71d-7a2cde8a14fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+-------+-----------+----+------+-------+-------------------------+--------------+-----+-------------+---------+---------------+\n",
      "| id|book_id|best_book_id|work_id|books_count|isbn|isbn13|authors|original_publication_year|original_title|title|language_code|image_url|small_image_url|\n",
      "+---+-------+------------+-------+-----------+----+------+-------+-------------------------+--------------+-----+-------------+---------+---------------+\n",
      "|  0|      0|           0|      0|          0| 700|   585|      0|                       21|           590|    0|         1084|        0|              0|\n",
      "+---+-------+------------+-------+-----------+----+------+-------+-------------------------+--------------+-----+-------------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look for null values in the dataset\n",
    "df_books.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_books.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6a054b8-ce12-46af-b288-07aad58b665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(language_code='\"\"A Man Named Dave\"\"\"'),\n",
       " Row(language_code='fre'),\n",
       " Row(language_code='en'),\n",
       " Row(language_code='en-CA'),\n",
       " Row(language_code='rus'),\n",
       " Row(language_code='ind'),\n",
       " Row(language_code='per'),\n",
       " Row(language_code=None),\n",
       " Row(language_code='nor'),\n",
       " Row(language_code='pol'),\n",
       " Row(language_code='vie'),\n",
       " Row(language_code='ara'),\n",
       " Row(language_code='por'),\n",
       " Row(language_code='swe'),\n",
       " Row(language_code='mul'),\n",
       " Row(language_code='eng'),\n",
       " Row(language_code='jpn'),\n",
       " Row(language_code='nl'),\n",
       " Row(language_code='dan'),\n",
       " Row(language_code='en-GB'),\n",
       " Row(language_code='fil'),\n",
       " Row(language_code='tur'),\n",
       " Row(language_code='rum'),\n",
       " Row(language_code='ita'),\n",
       " Row(language_code='Bloody Jack (Bloody Jack, #1)'),\n",
       " Row(language_code='en-US'),\n",
       " Row(language_code='spa'),\n",
       " Row(language_code='ger')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the different values for the books languages\n",
    "df_books.select('language_code').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f7164-3189-4edd-b0ca-6c4a5f2015e1",
   "metadata": {},
   "source": [
    "### Prepare the `books` dataset\n",
    "\n",
    "Based on the data exploration, we will need to clean the data following these steps:\n",
    "* **Delete** unncessary or too malformed **columns** by dropping `id`, `best_book_id`, `work_id`, `books_count`, `isbn13`, `title`, `image_url`, and `small_image_url`.\n",
    "* **Delete** unusable **rows** having null values in `original_title` and `original_publication_year` columns.\n",
    "* **Filling** empty records replacing null values in the `isbn` column by \"0\".\n",
    "* **Normalizing** records of the `language_code` column values to ISO 639-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17326a08-2113-4e4b-8b94-e831ed9ec5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[book_id: string, isbn: string, authors: string, original_publication_year: string, original_title: string, language_code: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean the \"books\" dataset\n",
    "df_books_cleaned = df_books\\\n",
    "    .drop(\"id\", \"best_book_id\", \"work_id\", \"books_count\", \"isbn13\", \"title\", \"image_url\", \"small_image_url\")\\\n",
    "    .dropna(subset=[\"original_title\", \"original_publication_year\"])\\\n",
    "    .fillna(value=\"0\", subset=[\"isbn\"])\\\n",
    "    .withColumn('language_code',\\\n",
    "        when(df_books.language_code.isin([\"en\", \"en-CA\", \"en-GB\", \"en-US\"]), \"eng\")\\\n",
    "        .when((df_books.language_code.isNull()) | (length(df_books.language_code) != 3), \"und\")\\\n",
    "        .otherwise(df_books.language_code))\n",
    "display(df_books_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cbf7a-78c6-4ecf-ad5a-e249853c9267",
   "metadata": {},
   "source": [
    "### Verify the clean \"books\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4040bfe4-dc3b-4636-8bde-f9092b54618b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+-------------------------+--------------+-------------+\n",
      "|book_id|isbn|authors|original_publication_year|original_title|language_code|\n",
      "+-------+----+-------+-------------------------+--------------+-------------+\n",
      "|      0|   0|      0|                        0|             0|            0|\n",
      "+-------+----+-------+-------------------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check there is no more null data\n",
    "df_books_cleaned.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_books_cleaned.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d694a6ee-581e-4e06-9868-50285643a300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(language_code='fre'),\n",
       " Row(language_code='und'),\n",
       " Row(language_code='rus'),\n",
       " Row(language_code='ind'),\n",
       " Row(language_code='per'),\n",
       " Row(language_code='nor'),\n",
       " Row(language_code='pol'),\n",
       " Row(language_code='vie'),\n",
       " Row(language_code='ara'),\n",
       " Row(language_code='por'),\n",
       " Row(language_code='swe'),\n",
       " Row(language_code='eng'),\n",
       " Row(language_code='jpn'),\n",
       " Row(language_code='dan'),\n",
       " Row(language_code='fil'),\n",
       " Row(language_code='tur'),\n",
       " Row(language_code='rum'),\n",
       " Row(language_code='ita'),\n",
       " Row(language_code='spa'),\n",
       " Row(language_code='ger')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the different values for the books languages\n",
    "df_books_cleaned.select('language_code').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925cf34-38b1-45a5-9850-78423bdaf790",
   "metadata": {},
   "source": [
    "### Download and check the \"ratings\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c905d5eb-0822-4661-8d31-a01360196a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[book_id: string, user_id: string, rating: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading \"ratings\" data from the S3 container\n",
    "df_ratings = spark.read.options(header='true').csv('s3a://spark-notebook-s3-container/ratings.csv')\n",
    "display(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed12ce3c-25d8-4067-8cff-c348844b9783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|book_id|user_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|    314|     5|\n",
      "|      1|    439|     3|\n",
      "|      1|    588|     5|\n",
      "|      1|   1169|     4|\n",
      "|      1|   1185|     4|\n",
      "|      1|   2077|     4|\n",
      "|      1|   2487|     4|\n",
      "|      1|   2900|     5|\n",
      "|      1|   3662|     4|\n",
      "|      1|   3922|     5|\n",
      "+-------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at a couple of ratings\n",
    "df_ratings.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b796150-9212-48bd-8fdd-c24851d75f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|book_id|user_id|rating|\n",
      "+-------+-------+------+\n",
      "|      0|      0|     0|\n",
      "+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look for null values in the dataset\n",
    "df_ratings.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_ratings.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671813cc-e9ec-4de9-bc4f-5adfdd05efd5",
   "metadata": {},
   "source": [
    "### Prepare the `ratings` dataset\n",
    "\n",
    "Based on the data exploration, we will need to clean the data with a single step:\n",
    "* **Delete** unncessary **columns** by dropping `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5fb9161-ad2d-4658-8987-e43c215fb78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[book_id: string, rating: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean the \"ratings\" dataset\n",
    "df_ratings_cleaned = df_ratings.drop(\"user_id\")\n",
    "display(df_ratings_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea97af-c6c5-4754-9ad6-1ade2307a934",
   "metadata": {},
   "source": [
    "### Aggregate the data\n",
    "\n",
    "For the data aggregation, let's try another style of data processing with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fd03f71-2434-4513-a1fe-78e4ee74069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create views\n",
    "df_books_cleaned.createOrReplaceTempView(\"books\")\n",
    "df_ratings_cleaned.createOrReplaceTempView(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b29fa2d-a3ea-4afc-bb91-0e789a805b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, title: string, authors: string, year: int, rating: double, language_code: string, isbn: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join data\n",
    "df_books_ratings = spark.sql(\"\"\"\n",
    "with books_ratings as (\n",
    "select cast(b.book_id as integer) as id, original_title as title, authors, cast(original_publication_year as integer) as year, round(avg(rating),2) as rating, language_code, isbn\n",
    "from books b, ratings r\n",
    "where b.book_id = r.book_id\n",
    "group by b.book_id, isbn, authors, original_publication_year, original_title, language_code)\n",
    "\n",
    "select * from books_ratings order by rating desc\n",
    "\"\"\")\n",
    "display(df_books_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d925e747-86f6-4b9d-9ca2-e103a8c15461",
   "metadata": {},
   "source": [
    "### Check the aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12e4e65a-a6ec-4246-b2f3-8e00e0470d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------------------------------------------+--------------------------------------+----+------+-------------+----------+\n",
      "|id  |title                                                   |authors                               |year|rating|language_code|isbn      |\n",
      "+----+--------------------------------------------------------+--------------------------------------+----+------+-------------+----------+\n",
      "|9566|Still Life with Woodpecker                              |Tom Robbins                           |1980|4.78  |eng          |184243022X|\n",
      "|4708|The Beautiful and Damned                                |F. Scott Fitzgerald                   |1922|4.66  |und          |743451503 |\n",
      "|9569|Villa Incognito                                         |Tom Robbins                           |2003|4.62  |und          |1842431021|\n",
      "|9531|Peter and the Shadow Thieves                            |Dave Barry, Ridley Pearson, Greg Call |2006|4.56  |eng          |078683787X|\n",
      "|3885|The Taste of Home Cookbook                              |Janet Briggs, Beth Wittlinger         |2006|4.55  |und          |898214971 |\n",
      "|2767|A People's History of the United States: 1492 to Present|Howard Zinn                           |1980|4.54  |eng          |60838655  |\n",
      "|5344|Hard Times: For These Times                             |Charles Dickens                       |1854|4.54  |eng          |321107217 |\n",
      "|2865|Girl with a Pearl Earring                               |Tracy Chevalier                       |1999|4.53  |eng          |452287022 |\n",
      "|976 |Deception Point                                         |Dan Brown                             |2001|4.5   |eng          |671027387 |\n",
      "|9712|El amor en los tiempos del cólera                       |Gabriel García Márquez, Edith Grossman|1985|4.5   |eng          |140003468X|\n",
      "+----+--------------------------------------------------------+--------------------------------------+----+------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at a couple of records\n",
    "df_books_ratings.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc6530-9bfb-4cbb-bb22-d40904721749",
   "metadata": {},
   "source": [
    "### Save the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2102e-1069-45fa-a1fb-6a5f4d2eae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the result to parquet\n",
    "df_books_ratings.write.mode(\"overwrite\").parquet(\"s3a://spark-notebook-s3-container/books_ratings.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcf8e8-375f-4deb-b845-108ac625c97f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark DP1-1",
   "language": "python",
   "name": "spark_python_odp_spark_operator_dp1_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
